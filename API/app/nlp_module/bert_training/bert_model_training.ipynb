{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:46:15.495727Z",
     "start_time": "2025-03-13T16:46:15.385581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Detect device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ],
   "id": "7a4c84f5ef0346b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:46:17.427602Z",
     "start_time": "2025-03-13T16:46:15.505727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset_name = \"normalized_wweia_meal_natural\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"../data/normalized/\"+dataset_name+\".csv\")\n",
    "\n",
    "# Define text and numerical features\n",
    "text_column = \"meal_description\"\n",
    "macro_columns = [\"carb\", \"protein\", \"fat\", \"energy\"]\n",
    "\n",
    "# Convert dataframe to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def tokenize_and_format(examples):\n",
    "    tokens = tokenizer(examples[text_column], padding=\"max_length\", truncation=True)\n",
    "\n",
    "    # Convert labels into a list of lists (shape: [batch_size, 4])\n",
    "    labels = torch.tensor([[examples[col][i] for col in macro_columns] for i in range(len(examples[text_column]))],\n",
    "                          dtype=torch.float32)\n",
    "\n",
    "    tokens[\"labels\"] = labels  # Attach labels correctly\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Apply tokenization\n",
    "dataset = dataset.map(tokenize_and_format, batched=True)\n",
    "\n",
    "# Split into train & test sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "# ✅ Ensure the dataset format is correct and move it to the correct device\n",
    "train_data.set_format(type=\"torch\")\n",
    "test_data.set_format(type=\"torch\")\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5532 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92f69ff505e04bfb9c81331c08fc6d87"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:46:23.637740Z",
     "start_time": "2025-03-13T16:46:17.467604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(macro_columns)  # Now predicting 4 continuous values (carb, protein, fat, energy)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"regression\"\n",
    ")\n",
    "\n",
    "# Move model to the correct device\n",
    "model.to(device)"
   ],
   "id": "a1edc830bf741c20",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:46:23.697782Z",
     "start_time": "2025-03-13T16:46:23.677747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "\n",
    "class MSETrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\").to(torch.float32).to(device)  # ✅ Move labels to correct device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}  # ✅ Move all inputs to device\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ],
   "id": "64aeca9255131338",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:10:24.567374Z",
     "start_time": "2025-03-13T16:46:23.697782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_\" + dataset_name,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs_\" + dataset_name\n",
    ")\n",
    "\n",
    "trainer = MSETrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "58e7f779069a5a5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2770' max='2770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2770/2770 23:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.005531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.005059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.003262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.002858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.002896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2770, training_loss=0.0051910255144649465, metrics={'train_runtime': 1434.6549, 'train_samples_per_second': 15.422, 'train_steps_per_second': 1.931, 'total_flos': 5821436634624000.0, 'train_loss': 0.0051910255144649465, 'epoch': 5.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T17:10:25.110739Z",
     "start_time": "2025-03-13T17:10:24.607673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "scaler = joblib.load(\"../pre_processing/scaler_nutrition.pkl\")\n",
    "\n",
    "def predict_macros(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # ✅ Move input tensors to the correct device\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    predicted_macros = outputs.logits.detach().cpu().numpy()[0]  # ✅ Move predictions back to CPU before converting to NumPy\n",
    "\n",
    "    predicted_macros = scaler.inverse_transform(predicted_macros)\n",
    "\n",
    "    return {\n",
    "        \"carbs\": predicted_macros[0],\n",
    "        \"protein\": predicted_macros[1],\n",
    "        \"fat\": predicted_macros[2],\n",
    "        \"energy\": predicted_macros[3]\n",
    "    }\n",
    "\n",
    "model.save_pretrained(\"bert_nutrition_classifier_\" + dataset_name)\n",
    "tokenizer.save_pretrained(\"bert_nutrition_classifier_\" + dataset_name)"
   ],
   "id": "1881c7653fdc39f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert_nutrition_classifier_normalized_wweia_meal_natural\\\\tokenizer_config.json',\n",
       " 'bert_nutrition_classifier_normalized_wweia_meal_natural\\\\special_tokens_map.json',\n",
       " 'bert_nutrition_classifier_normalized_wweia_meal_natural\\\\vocab.txt',\n",
       " 'bert_nutrition_classifier_normalized_wweia_meal_natural\\\\added_tokens.json',\n",
       " 'bert_nutrition_classifier_normalized_wweia_meal_natural\\\\tokenizer.json')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
